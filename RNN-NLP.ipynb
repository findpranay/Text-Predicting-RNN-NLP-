{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3161c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ddc35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Project Gutenberg eBook of The Adventures of Sherlock Holmes, by Arthur Conan Doyle This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are lo'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"C:/Users/Yaniv/Desktop/cnn/The Adventures of Sherlock Holme.txt\", \"r\", encoding = \"utf8\")\n",
    "\n",
    "# store file in list\n",
    "lines = []\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "\n",
    "# Convert list to string\n",
    "data = \"\"\n",
    "for i in lines:\n",
    "  data = ' '. join(lines) \n",
    "\n",
    "#replace unnecessary stuff with space\n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
    "\n",
    "#remove unnecessary spaces \n",
    "data = data.split()\n",
    "data = ' '.join(data)\n",
    "data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa83db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20100"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97896735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "t=tokenizer.fit_on_texts([data])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580a747c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 114, 91, 83, 2, 1, 115, 2, 92, 18, 35, 138, 139, 140, 29]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function\n",
    "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "sequence_data[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abc78a2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3707"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequence_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faaa348d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65f75670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Length of sequences are:  3704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  1, 114,  91,  83],\n",
       "       [114,  91,  83,   2],\n",
       "       [ 91,  83,   2,   1],\n",
       "       [ 83,   2,   1, 115],\n",
       "       [  2,   1, 115,   2],\n",
       "       [  1, 115,   2,  92],\n",
       "       [115,   2,  92,  18],\n",
       "       [  2,  92,  18,  35],\n",
       "       [ 92,  18,  35, 138],\n",
       "       [ 18,  35, 138, 139]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = []\n",
    "\n",
    "for i in range(3, len(sequence_data)):\n",
    "    words = sequence_data[i-3:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "print(\"The Length of sequences are: \", len(sequences))\n",
    "sequences = np.array(sequences)\n",
    "sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb21af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0:3])\n",
    "    y.append(i[3])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc58e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[  1 114  91]\n",
      " [114  91  83]\n",
      " [ 91  83   2]\n",
      " [ 83   2   1]\n",
      " [  2   1 115]\n",
      " [  1 115   2]\n",
      " [115   2  92]\n",
      " [  2  92  18]\n",
      " [ 92  18  35]\n",
      " [ 18  35 138]]\n",
      "Response:  [ 83   2   1 115   2  92  18  35 138 139]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data: \", X[:10])\n",
    "print(\"Response: \", y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d54c25af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb463ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=3))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3266948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 3, 10)             12240     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 3, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1224)              1225224   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,286,464\n",
      "Trainable params: 14,286,464\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2513a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.4856\n",
      "Epoch 1: loss improved from inf to 6.48563, saving model to next_words.h5\n",
      "58/58 [==============================] - 20s 226ms/step - loss: 6.4856\n",
      "Epoch 2/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.0864\n",
      "Epoch 2: loss improved from 6.48563 to 6.08639, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 6.0864\n",
      "Epoch 3/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 6.0055\n",
      "Epoch 3: loss improved from 6.08639 to 6.00552, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 6.0055\n",
      "Epoch 4/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.9181\n",
      "Epoch 4: loss improved from 6.00552 to 5.91810, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 5.9181\n",
      "Epoch 5/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.7582\n",
      "Epoch 5: loss improved from 5.91810 to 5.75819, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 225ms/step - loss: 5.7582\n",
      "Epoch 6/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.5023\n",
      "Epoch 6: loss improved from 5.75819 to 5.50228, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 229ms/step - loss: 5.5023\n",
      "Epoch 7/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 5.2120\n",
      "Epoch 7: loss improved from 5.50228 to 5.21202, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 248ms/step - loss: 5.2120\n",
      "Epoch 8/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.9593\n",
      "Epoch 8: loss improved from 5.21202 to 4.95930, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 245ms/step - loss: 4.9593\n",
      "Epoch 9/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.7032\n",
      "Epoch 9: loss improved from 4.95930 to 4.70320, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 243ms/step - loss: 4.7032\n",
      "Epoch 10/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.4612\n",
      "Epoch 10: loss improved from 4.70320 to 4.46122, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 233ms/step - loss: 4.4612\n",
      "Epoch 11/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.2270\n",
      "Epoch 11: loss improved from 4.46122 to 4.22699, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 247ms/step - loss: 4.2270\n",
      "Epoch 12/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 4.0253\n",
      "Epoch 12: loss improved from 4.22699 to 4.02525, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 239ms/step - loss: 4.0253\n",
      "Epoch 13/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.7949\n",
      "Epoch 13: loss improved from 4.02525 to 3.79490, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 237ms/step - loss: 3.7949\n",
      "Epoch 14/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.5937\n",
      "Epoch 14: loss improved from 3.79490 to 3.59373, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 244ms/step - loss: 3.5937\n",
      "Epoch 15/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.4137\n",
      "Epoch 15: loss improved from 3.59373 to 3.41370, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 234ms/step - loss: 3.4137\n",
      "Epoch 16/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.2228\n",
      "Epoch 16: loss improved from 3.41370 to 3.22282, saving model to next_words.h5\n",
      "58/58 [==============================] - 15s 254ms/step - loss: 3.2228\n",
      "Epoch 17/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 3.0458\n",
      "Epoch 17: loss improved from 3.22282 to 3.04582, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 236ms/step - loss: 3.0458\n",
      "Epoch 18/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.8759\n",
      "Epoch 18: loss improved from 3.04582 to 2.87587, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 235ms/step - loss: 2.8759\n",
      "Epoch 19/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.7251\n",
      "Epoch 19: loss improved from 2.87587 to 2.72510, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 227ms/step - loss: 2.7251\n",
      "Epoch 20/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.5503\n",
      "Epoch 20: loss improved from 2.72510 to 2.55032, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 231ms/step - loss: 2.5503\n",
      "Epoch 21/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.3471\n",
      "Epoch 21: loss improved from 2.55032 to 2.34712, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 231ms/step - loss: 2.3471\n",
      "Epoch 22/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.2435\n",
      "Epoch 22: loss improved from 2.34712 to 2.24347, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 239ms/step - loss: 2.2435\n",
      "Epoch 23/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 2.0404\n",
      "Epoch 23: loss improved from 2.24347 to 2.04043, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 225ms/step - loss: 2.0404\n",
      "Epoch 24/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.9074\n",
      "Epoch 24: loss improved from 2.04043 to 1.90743, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 1.9074\n",
      "Epoch 25/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.8257\n",
      "Epoch 25: loss improved from 1.90743 to 1.82570, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 1.8257\n",
      "Epoch 26/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.7103\n",
      "Epoch 26: loss improved from 1.82570 to 1.71028, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 1.7103\n",
      "Epoch 27/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.5364\n",
      "Epoch 27: loss improved from 1.71028 to 1.53645, saving model to next_words.h5\n",
      "58/58 [==============================] - 15s 253ms/step - loss: 1.5364\n",
      "Epoch 28/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.4338\n",
      "Epoch 28: loss improved from 1.53645 to 1.43381, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 245ms/step - loss: 1.4338\n",
      "Epoch 29/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2901\n",
      "Epoch 29: loss improved from 1.43381 to 1.29009, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 238ms/step - loss: 1.2901\n",
      "Epoch 30/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.2534\n",
      "Epoch 30: loss improved from 1.29009 to 1.25337, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 238ms/step - loss: 1.2534\n",
      "Epoch 31/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.1432\n",
      "Epoch 31: loss improved from 1.25337 to 1.14316, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 241ms/step - loss: 1.1432\n",
      "Epoch 32/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 1.0306\n",
      "Epoch 32: loss improved from 1.14316 to 1.03057, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 231ms/step - loss: 1.0306\n",
      "Epoch 33/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.9705\n",
      "Epoch 33: loss improved from 1.03057 to 0.97045, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 233ms/step - loss: 0.9705\n",
      "Epoch 34/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.9106\n",
      "Epoch 34: loss improved from 0.97045 to 0.91059, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 233ms/step - loss: 0.9106\n",
      "Epoch 35/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.8662\n",
      "Epoch 35: loss improved from 0.91059 to 0.86615, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 234ms/step - loss: 0.8662\n",
      "Epoch 36/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.8202\n",
      "Epoch 36: loss improved from 0.86615 to 0.82018, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 230ms/step - loss: 0.8202\n",
      "Epoch 37/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.7456\n",
      "Epoch 37: loss improved from 0.82018 to 0.74564, saving model to next_words.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 13s 228ms/step - loss: 0.7456\n",
      "Epoch 38/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.6781\n",
      "Epoch 38: loss improved from 0.74564 to 0.67812, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 226ms/step - loss: 0.6781\n",
      "Epoch 39/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.5651\n",
      "Epoch 39: loss improved from 0.67812 to 0.56514, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 225ms/step - loss: 0.5651\n",
      "Epoch 40/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4999\n",
      "Epoch 40: loss improved from 0.56514 to 0.49993, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 229ms/step - loss: 0.4999\n",
      "Epoch 41/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4545\n",
      "Epoch 41: loss improved from 0.49993 to 0.45455, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 226ms/step - loss: 0.4545\n",
      "Epoch 42/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.4078\n",
      "Epoch 42: loss improved from 0.45455 to 0.40776, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 228ms/step - loss: 0.4078\n",
      "Epoch 43/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3963\n",
      "Epoch 43: loss improved from 0.40776 to 0.39627, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 227ms/step - loss: 0.3963\n",
      "Epoch 44/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3962\n",
      "Epoch 44: loss improved from 0.39627 to 0.39618, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3962\n",
      "Epoch 45/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3912\n",
      "Epoch 45: loss improved from 0.39618 to 0.39117, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3912\n",
      "Epoch 46/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3810\n",
      "Epoch 46: loss improved from 0.39117 to 0.38101, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3810\n",
      "Epoch 47/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3966\n",
      "Epoch 47: loss did not improve from 0.38101\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3966\n",
      "Epoch 48/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3877\n",
      "Epoch 48: loss did not improve from 0.38101\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3877\n",
      "Epoch 49/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3681\n",
      "Epoch 49: loss improved from 0.38101 to 0.36812, saving model to next_words.h5\n",
      "58/58 [==============================] - 14s 234ms/step - loss: 0.3681\n",
      "Epoch 50/50\n",
      "58/58 [==============================] - ETA: 0s - loss: 0.3433\n",
      "Epoch 50: loss improved from 0.36812 to 0.34334, saving model to next_words.h5\n",
      "58/58 [==============================] - 13s 227ms/step - loss: 0.3433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11460c4e0a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "12bdb738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('NLP.pickle','wb') as f:\n",
    "    pickle.dump(\"next_words.h5\",f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "618c564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NLPTokenizer.pickle','wb') as f:\n",
    "    pickle.dump(\"token.pkl\",f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "de27598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "columns = {\n",
    "    'data_columns' : [col.lower() for col in 'next_words.h5']\n",
    "}\n",
    "with open(\"NLPmodel.json\",\"w\") as f:\n",
    "    f.write(json.dumps(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9f74aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfaf58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sequence = tokenizer.texts_to_sequences([text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7849819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = load_model('next_words.h5')\n",
    "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
    "\n",
    "def Predict_Next_Words(model, tokenizer, text):\n",
    "  sequence = tokenizer.texts_to_sequences([text])\n",
    "  print(sequence)\n",
    "  sequence = np.array(sequence)\n",
    "  print(sequence)\n",
    "  preds = np.argmax(model.predict(sequence))\n",
    "  predicted_word = \"\"\n",
    "  \n",
    "  for key, value in tokenizer.word_index.items():\n",
    "      if value == preds:\n",
    "          predicted_word = key\n",
    "          break\n",
    "  \n",
    "  print(predicted_word)\n",
    "  return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7457d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your line: The Robert\n",
      "['The', 'Robert']\n",
      "[[1]]\n",
      "[[1]]\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 3) for input KerasTensor(type_spec=TensorSpec(shape=(None, 3), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "1/1 [==============================] - 1s 844ms/step\n",
      "august\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "  text = input(\"Enter your line: \")\n",
    "  \n",
    "  if text == \"0\":\n",
    "      print(\"Execution completed.....\")\n",
    "      break\n",
    "  \n",
    "  else:\n",
    "      try:\n",
    "          text = text.split(\" \")\n",
    "          text = text[-3:]\n",
    "          print(text)\n",
    "        \n",
    "          Predict_Next_Words(model, tokenizer, text)\n",
    "          \n",
    "      except Exception as e:\n",
    "        print(\"Error occurred: \",e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8ad5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59484500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
